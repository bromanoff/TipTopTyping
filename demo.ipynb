{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Environment\n",
    "\n",
    "## Description Lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "\n",
    "import cv2\n",
    "from PIL import ImageFont, ImageDraw, Image  \n",
    "import mediapipe as mp\n",
    "# from subprocess import call\n",
    "import numpy as np\n",
    "from trie import Trie, TrieNode\n",
    "import math\n",
    "from playsound import playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numerical tuple\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/romanbeier/Documents/Education/Master/TU Wien/04-Master-SoSe23/Masterthesis/Code/tip-top-typing/demo.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/romanbeier/Documents/Education/Master/TU%20Wien/04-Master-SoSe23/Masterthesis/Code/tip-top-typing/demo.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m frame \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/romanbeier/Documents/Education/Master/TU%20Wien/04-Master-SoSe23/Masterthesis/Code/tip-top-typing/demo.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# PIL handover for text on image\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/romanbeier/Documents/Education/Master/TU%20Wien/04-Master-SoSe23/Masterthesis/Code/tip-top-typing/demo.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m cv2_img_rgb \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(img,cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/romanbeier/Documents/Education/Master/TU%20Wien/04-Master-SoSe23/Masterthesis/Code/tip-top-typing/demo.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pil_img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(cv2_img_rgb)  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romanbeier/Documents/Education/Master/TU%20Wien/04-Master-SoSe23/Masterthesis/Code/tip-top-typing/demo.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m draw \u001b[39m=\u001b[39m ImageDraw\u001b[39m.\u001b[39mDraw(pil_img)  \n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numerical tuple\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "# Video setup\n",
    "videoCap = cv2.VideoCapture(0)\n",
    "img = videoCap.read()\n",
    "lastFrameTime = 0\n",
    "frame = 0\n",
    "\n",
    "# PIL handover for text on image\n",
    "cv2_img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) \n",
    "pil_img = Image.fromarray(cv2_img_rgb)  \n",
    "draw = ImageDraw.Draw(pil_img)  \n",
    "font = ImageFont.truetype(\"fonts/RobotoMono-Regular.ttf\", 50) # use a truetype font \n",
    "cv2_img_processed = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand Detection Setup\n",
    "handSolution = mp.solutions.hands\n",
    "hands = handSolution.Hands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palm facing mental model \n",
    "CHAR_DICT = {\"Right\": {\n",
    "    \t\t\t0: [\"qwert\", False],\n",
    "                1: [\"asdf\", False],\n",
    "                2: [\"zxc\", False],\n",
    "                3: [\"SPACE\", False]},\n",
    "       \t\t\"Left\": {\n",
    "             \t0: [\"yuiop\", False],\n",
    "                1: [\"ghjkl\", False],\n",
    "                2: [\"vbnm\", False],\n",
    "                3: [\"<-\", False]}}\n",
    "\n",
    "LANGUAGE = [\"hut\", \"haus\", \"haut\", \"mut\", \"maus\", \"maut\", \"mann\", \"hello\", \"world\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trie Datastruture to store and query language\n",
    "trie = Trie()\n",
    "trie.extend(LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing setup\n",
    "input_msg = []\n",
    "output_msg = \"\"\n",
    "line_pos_x = [400, 430]\n",
    "\n",
    "# pull random phrase from phrases2.txt and save it in a variable\n",
    "with open(\"phrases/phrases2.txt\", \"r\") as f:\n",
    "    phrases = f.readlines()\n",
    "    test_phrase = phrases[np.random.randint(0, len(phrases))].strip()\n",
    "\n",
    "phrase_chars = {}\n",
    "for idx, symbol in enumerate(test_phrase):\n",
    "    phrase_chars[idx] = [idx * 30, symbol, (105, 105, 105)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\n",
    "def distance(pos1, pos2): #pos = (x, y)\n",
    "    Distance = int(math.sqrt(((pos2[0] - pos1[0]) * (pos2[0] - pos1[0])) + ((pos2[1] - pos1[1]) * (pos2[1] - pos1[1]))))\n",
    "    return Distance\n",
    "\n",
    "def write_char(hand, target):\n",
    "    global input_msg\n",
    "    global output_msg\n",
    "    if not target == 3: #pinky\n",
    "        input_msg.append(CHAR_DICT[hand][target][0])\n",
    "        line_pos_x[0] += 30\n",
    "        line_pos_x[1] += 30\n",
    "        CHAR_DICT[hand][target][1] = True\n",
    "        if len(input_msg) >= 0: # coloring displayed sentence and soundFX when user types\n",
    "            if test_phrase[len(input_msg)-1] in input_msg[len(input_msg)-1]:\n",
    "                phrase_chars[len(input_msg)-1][2] = (0, 255, 0)\n",
    "                playsound(\"/Users/romanbeier/Documents/Education/Master/TU Wien/04-Master-SoSe23/Masterthesis/Code/tip-top-typing/soundFX/key_press_click.caf\")\n",
    "            else:\n",
    "                phrase_chars[len(input_msg)-1][2] = (255, 0, 0)\n",
    "                playsound(\"/Users/romanbeier/Documents/Education/Master/TU Wien/04-Master-SoSe23/Masterthesis/Code/tip-top-typing/soundFX/keyboard_press_normal.caf\")\n",
    "    else:\n",
    "        match hand:\n",
    "            case \"Right\":\n",
    "                input_msg += \" \"\n",
    "                output_msg += \" \"\n",
    "                line_pos_x[0] += 30\n",
    "                line_pos_x[1] += 30\n",
    "                playsound(\"soundFX/key_press_click.caf\")\n",
    "                CHAR_DICT[hand][target][1] = True\n",
    "            case \"Left\":\n",
    "                CHAR_DICT[hand][target][1] = True\n",
    "                try:\n",
    "                    phrase_chars[len(input_msg)-1][2] = (105, 105, 105) # turn deleted character gray again\n",
    "                    input_msg = input_msg[:-1]\n",
    "                    output_msg = output_msg[:-1]\n",
    "                    line_pos_x[0] -= 30\n",
    "                    line_pos_x[1] -= 30\n",
    "                    playsound(\"soundFX/key_press_delete.caf\")\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "    print(f\"Input Message: {input_msg}\")\n",
    "    \n",
    "def char_written(hand, target):\n",
    "    if CHAR_DICT[hand][target][1] == False:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def vector(t1, t2):\n",
    "    x = t2[0] - t1[0]\n",
    "    y = t2[1] - t1[1]\n",
    "    z = t2[2] - t1[2]\n",
    "    return (x, y, z)\n",
    "\n",
    "def print_phrase(phrase): # for printing self written phrase on image\n",
    "    position = 0\n",
    "    for char in phrase:\n",
    "        cv2.putText(cv2_img_processed, char, ((400 + position), 940), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 0), 2)\n",
    "        position += 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Loop\n",
    "\n",
    "while True:\n",
    "    frame += 1\n",
    "    success, img = videoCap.read() #reading image\n",
    "    # img = cv2.flip(img, 1) #mirror image\n",
    "    # img = cv2.flip(img, -1) #flip image in both directions\n",
    "    \n",
    "    # UI\n",
    "    cv2.rectangle(img, (200,880), (1720,980), (255,255,255), -1) #draw rectangle for text\n",
    "    # cv2.putText(img, test_phrase, (400, 940), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 0), 2) #put test phrase on image\n",
    "    \n",
    "    for char in phrase_chars.values():\n",
    "        draw.text(((400 + char[0]), 890), char[1], font=font, fill=char[2])\n",
    "    cv2.line(cv2_img_processed,(line_pos_x[0], 960),(line_pos_x[1], 960),(105,105,105),5) #draw line for input message\n",
    "    \n",
    "    #fps calculations\n",
    "    thisFrameTime = time.time()\n",
    "    fps = 1 / (thisFrameTime - lastFrameTime)\n",
    "    lastFrameTime = thisFrameTime\n",
    "    cv2.putText(cv2_img_processed, f'FPS:{int(fps)}',\n",
    "        (20, 70),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    #wpm calculations\n",
    "    # TODO: Implement wpm calculations\n",
    "    wpm = 0\n",
    "    cv2.putText(cv2_img_processed, f'FPS:{int(fps)}',\n",
    "        (20, 70),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    #recognize hands from out image\n",
    "    recHands = hands.process(img)\n",
    "    multi_hand_landmarks_list = recHands.multi_hand_landmarks\n",
    "    multi_handedness_list = recHands.multi_handedness\n",
    "    \n",
    "    if recHands.multi_hand_landmarks:   # if there are any hands recognized\n",
    "        \n",
    "        for idx in range(len(multi_hand_landmarks_list)):     # for each hand recognized\n",
    "            hand_landmarks = multi_hand_landmarks_list[idx]   # get the hand landmarks\n",
    "            handedness = multi_handedness_list[idx]           # get the handedness\n",
    "            hand_label = handedness.classification[0].label   # get the hand label (left or right)\n",
    "            \n",
    "            height, width, _ = img.shape\n",
    "            thumb_pos = (hand_landmarks.landmark[4].x * width, hand_landmarks.landmark[4].y * height)\n",
    "            # print(\"thumb y-pos: \", hand_landmarks.landmark[4].y * height, \"thumb x-pos: \", hand_landmarks.landmark[4].x * width, \"thumb-z-pos: \", hand_landmarks.landmark[4].z)\n",
    "            \n",
    "            # Calculate thumb top position\n",
    "            thumb_tip_3d = (hand_landmarks.landmark[4].x * width, hand_landmarks.landmark[4].y * height, hand_landmarks.landmark[4].z)\n",
    "            thumb_ip_3d = (hand_landmarks.landmark[3].x * width, hand_landmarks.landmark[3].y * height, hand_landmarks.landmark[3].z)\n",
    "            thumb_vector = vector(thumb_ip_3d, thumb_tip_3d)\n",
    "            thumb_vector = (thumb_vector[0] * 0.3, thumb_vector[1] * 0.3, thumb_vector[2] * 0.3) # scale vector\n",
    "            thumb_top = ((thumb_tip_3d[0] + thumb_vector[0]), (thumb_tip_3d[1] + thumb_vector[1]), (thumb_tip_3d[2] + thumb_vector[2]))\n",
    "            \n",
    "            # Reset input message when pinky tips touch\n",
    "            if hand_label == \"Left\":\n",
    "                left_pinky_tip_pos = ((hand_landmarks.landmark[20].x * width), (hand_landmarks.landmark[20].y * height))\n",
    "            if hand_label == \"Right\":\n",
    "                right_pinky_tip_pos = ((hand_landmarks.landmark[20].x * width), (hand_landmarks.landmark[20].y * height))\n",
    "            \n",
    "            try: #exception handling for first iteration with uncomputed right pinky tip\n",
    "                if distance(left_pinky_tip_pos, right_pinky_tip_pos) <= 20 and not input_msg == []:\n",
    "                    input_msg = []\n",
    "                    for char in phrase_chars:\n",
    "                        phrase_chars[char][2] = (0, 0, 0)\n",
    "                    cv2.putText(cv2_img_processed, \"Input message cleared\", (800, 1030), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "                    line_pos_x = [400, 430]\n",
    "                    print(\"Input message cleared\")\n",
    "                    playsound(\"soundFX/keyboard_press_clear.caf\")\n",
    "                    time.sleep(0.1)\n",
    "            except NameError:\n",
    "                continue\n",
    "            \n",
    "            # Thumb Annotations\n",
    "            cv2.circle(cv2_img_processed, (int(thumb_top[0]), int(thumb_top[1])), 5, (0, 0, 255), -1) # Draw Circles on elongatetd thumb top position\n",
    "                        \n",
    "            \n",
    "            # Calculate landmark positions for thumb and finger tips, except pinky [from:to:increment]  \n",
    "            for idx, point in enumerate(hand_landmarks.landmark[8:21:4]):      \n",
    "                h, w, c = img.shape \n",
    "                # landmark_pos = (int(point.x * w), int(point.y * h))\n",
    "                landmark_pos = (point.x * w, point.y * h)\n",
    "                \n",
    "                cv2.putText(cv2_img_processed, CHAR_DICT[hand_label][idx][0], (int(landmark_pos[0]), int(landmark_pos[1])), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 255), 2)\n",
    "                \n",
    "                \n",
    "                # TODO: Add a variable threshold for distance between thumb and finger tips based on possible next characters, PERMUTATIONS?\n",
    "                # TODO: OR: Only allow characters that are child nodes of chars in input_msg\n",
    "                \n",
    "                if distance(thumb_top, landmark_pos) <= 70 and not char_written(hand_label, idx):\n",
    "                    write_char(hand_label, idx)\n",
    "                    if len(input_msg) == len(test_phrase): # if input message is as long as test phrase, check if correct                 \n",
    "                        with open(\"phrases/phrases2.txt\", \"r\") as f:\n",
    "                            phrases = f.readlines()\n",
    "                            test_phrase = phrases[np.random.randint(0, len(phrases))].strip()\n",
    "                        input_msg = []\n",
    "                        phrase_chars = {}\n",
    "                        line_pos_x = [400, 430]\n",
    "                        for idx, symbol in enumerate(test_phrase):\n",
    "                            phrase_chars[idx] = [idx * 30, symbol, (105, 105, 105)]\n",
    "\n",
    "                    \n",
    "                elif distance(thumb_top, landmark_pos) <= 70 and char_written:\n",
    "                    pass\n",
    "                                    \n",
    "                elif distance(thumb_top, landmark_pos) > 120 and char_written(hand_label, idx):\n",
    "                    CHAR_DICT[hand_label][idx][1] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Video running\n",
    "cv2.imshow(\"Cam Output\", img)\n",
    "cv2.waitKey(1)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
